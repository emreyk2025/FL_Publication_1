{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SmartMeter 2012 Simulations\n",
    "\n",
    "Simulations with varying number of LSTM neurons (x2, x4, and x10) and cross enthropy as the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Input_prep import train_val_test\n",
    "import os\n",
    "from tensorflow.keras import losses\n",
    "from keras import layers, Sequential, optimizers\n",
    "from utils import create_sequences\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists for neuron multipliers and loss functions\n",
    "neurons_list = [2, 4, 10]\n",
    "\n",
    "loss_functions = {\n",
    "    'MSE': losses.MeanSquaredError(),\n",
    "    'MAE': losses.MeanAbsoluteError(),\n",
    "    'Huber (Î´=1.5)': losses.Huber(delta=1.5),\n",
    "    'MSLE': losses.MeanSquaredLogarithmicError(),\n",
    "    'Cross Entropy': losses.BinaryCrossentropy()\n",
    "}\n",
    "\n",
    "data_points_drop = [0.1, 0.2, 0.3]\n",
    "\n",
    "VERBOSE = 1\n",
    "\n",
    "num_meters_list = [50, 100, 4411]\n",
    "\n",
    "models = ['LSTM_Simple', 'LSTM_stacked']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "dataset = pd.read_csv(\"D:/FL Publication/Code_new/FL_Publication_1/Current_Simulations/Data_Preprocessing/Preprocessed_data/SmartMeter_2013_hourly.csv\")\n",
    "\n",
    "data_splits = {\n",
    "    'train': (dataset[\"train\"].drop(columns=['KWH/hh (per hour)']),\n",
    "            dataset[\"train\"]['KWH/hh (per hour)']),\n",
    "    'validation': (dataset[\"validation\"].drop(columns=['KWH/hh (per hour)']),\n",
    "                dataset[\"validation\"]['KWH/hh (per hour)']),\n",
    "    'test': (dataset[\"test\"].drop(columns=['KWH/hh (per hour)']),\n",
    "            dataset[\"test\"]['KWH/hh (per hour)'])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "\n",
    "def get_LSTM_Simple(time_steps, num_features, learning_rate, neurons=60, loss_function='mean_squared_error'):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape=(time_steps, num_features)))\n",
    "    model.add(layers.LSTM(neurons, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    optimizer=optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=['mean_absolute_error'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_LSTM_stacked(time_steps, num_features, learning_rate, neurons=60, loss_function='mean_squared_error'):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape=(time_steps, num_features)))\n",
    "    model.add(layers.LSTM(neurons//2, return_sequences=True)) \n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.LSTM(neurons//3, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.LSTM(neurons//4, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.LSTM(neurons//12))  \n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=['mean_absolute_error'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the each model in a loop  with the different loss functions and neuron multipliers\n",
    "for num_meters in num_meters_list:\n",
    "    for neuron_multiplier in neurons_list:\n",
    "        for loss_function in loss_functions:\n",
    "            for data_dropout in data_points_drop: # Added data dropout loop\n",
    "                for model_name in models:\n",
    "                    if model_name == 'LSTM_Simple':\n",
    "                        data_splits = {\n",
    "                            'train': (df_dataset[\"train\"].drop(columns=[target_column]),\n",
    "                                    df_dataset[\"train\"][target_column]),\n",
    "                            'validation': (df_dataset[\"validation\"].drop(columns=[target_column]),\n",
    "                                        df_dataset[\"validation\"][target_column]),\n",
    "                            'test': (df_dataset[\"test\"].drop(columns=[target_column]),\n",
    "                                    df_dataset[\"test\"][target_column])\n",
    "                        }\n",
    "                    \n",
    "                        # Convert all data to numpy arrays\n",
    "                        for split in data_splits:          \n",
    "                            data_splits[split] = (np.asarray(data_splits[split][0]),\n",
    "                                                np.asarray(data_splits[split][1]))\n",
    "                            \n",
    "                        # Extract features and labels for training, validation, and testing\n",
    "                        x_train, y_train = data_splits['train']\n",
    "                        x_val, y_val = data_splits['validation']\n",
    "                        x_test, y_test = data_splits['test']\n",
    "                        \n",
    "                        # Apply data dropout if specified\n",
    "                        if data_dropout > 0:\n",
    "                            mask = np.random.rand(*x_train.shape) > data_dropout\n",
    "                            x_train = x_train * mask\n",
    "                        \n",
    "                        if model_name in ['simple_LSTM', 'stacked_LSTM', 'bidirectional_LSTM', 'GRU']:\n",
    "                            try:\n",
    "                                # Create sequences\n",
    "                                x_train_seq, y_train_seq = create_sequences(\n",
    "                                    X = x_train, y = y_train, time_steps = time_steps)\n",
    "                                x_val_seq, y_val_seq = create_sequences(\n",
    "                                    X = x_val, y = y_val, time_steps = time_steps)\n",
    "                                x_test_seq, y_test_seq = create_sequences(\n",
    "                                    X = x_test, y = y_test, time_steps = time_steps)\n",
    "\n",
    "                                num_features = x_train_seq.shape[2]\n",
    "\n",
    "                                # Initialize model with time_steps and num_features\n",
    "                                model = model_func(time_steps=time_steps, num_features=num_features, **model_params)\n",
    "\n",
    "                                # Convert data to tensors\n",
    "                                x_train_tensor = convert_to_tensor(x_train_seq, dtype=tf.float32)\n",
    "                                y_train_tensor = convert_to_tensor(y_train_seq, dtype=tf.float32)\n",
    "                                x_val_tensor = convert_to_tensor(x_val_seq, dtype=tf.float32)\n",
    "                                y_val_tensor = convert_to_tensor(y_val_seq, dtype=tf.float32)\n",
    "                                x_test_tensor = convert_to_tensor(x_test_seq, dtype=tf.float32)\n",
    "                                y_test_tensor = convert_to_tensor(y_test_seq, dtype=tf.float32)\n",
    "\n",
    "                                # Add early stopping callback\n",
    "                                early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                                    monitor='val_loss',\n",
    "                                    patience=3,\n",
    "                                    restore_best_weights=True,\n",
    "                                )\n",
    "                                \n",
    "                                # Train the model\n",
    "                                history = model.fit(\n",
    "                                    x_train_tensor,\n",
    "                                    y_train_tensor,\n",
    "                                    validation_data=(x_val_tensor, y_val_tensor),\n",
    "                                    epochs=10,\n",
    "                                    batch_size=512,\n",
    "                                    verbose=verbose,\n",
    "                                    callbacks=[early_stopping]\n",
    "                                )\n",
    "                                \n",
    "                                training_time = time.time() - start_time\n",
    "                                results = model.evaluate(x_test_tensor, y_test_tensor, verbose=verbose)\n",
    "                            \n",
    "                                return results, history.history, training_time, num_meters\n",
    "\n",
    "                        model = get_LSTM_Simple(time_steps, num_features, learning_rate, 60 * neuron_multiplier, loss_function)\n",
    "                    elif model_name == 'LSTM_stacked':\n",
    "                        model = get_LSTM_stacked(time_steps, num_features, learning_rate, 60 * neuron_multiplier, loss_function)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
