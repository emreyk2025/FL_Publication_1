{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chosen Data Sets:\n",
    "\n",
    "GoiEner Data Set\n",
    "\n",
    "Smart Grid Smart City Customer Trial Data Set\n",
    "\n",
    "METER UK Household Electricity and Activity Survey, 2016-2019\n",
    "\n",
    "Original SmartMeter dataset (year 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from electricity use, there are some other interesting information in various csv files.\n",
    "\n",
    "There is a seperate CSV file for Type of  Home Area Network appliance or room electricity usage readings. This can be a subject of different study or it can be included in our publication as an additional analysis. There is a unique customer ID and names of different home appliances / home areas and these can be used for a federated network (Considering each home appliance / home area as a node in the federated framework), by labeling each home appliance / home area with a number.\n",
    "\n",
    "Apart from this, there are csv files containing information about households (demographics, lifestyle and etc.). This can be used as additional features to improve forecasting (May be the algorithms / models employed will learn some patterns from these, people with certain lifestyle, age group, location etc behaving in similar patterns etc.) Also, this can be used to assess fairness aspects of the models.\n",
    "\n",
    "Lastly there are 3 CSV files for offers, peak events, and peak event responses. Consider also using these information to improve forecasting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import holidays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x1e879857450>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask.config.set(temporary_directory='D:/Dask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's load the data set.\n",
    "\n",
    "file_path_sgsc = 'D:/FL Publication/Datasets for the Publication/Australia - Smart-Grid Smart-City Customer Trial Data/electricity_use_interval_readings/electricity_use_interval_readings.csv'\n",
    "sgsc_data = dd.read_csv(file_path_sgsc, assume_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CUSTOMER_ID', 'READING_DATETIME', ' CALENDAR_KEY', ' EVENT_KEY',\n",
      "       ' GENERAL_SUPPLY_KWH', ' CONTROLLED_LOAD_KWH', ' GROSS_GENERATION_KWH',\n",
      "       ' NET_GENERATION_KWH', ' OTHER_KWH'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(sgsc_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First check number of rows to make sure that data is complete.\n",
    "# Get the number of rows and columns in the Dask DataFrame\n",
    "num_rows, num_cols = sgsc_data.shape\n",
    "# Compute the number of rows\n",
    "num_rows = sgsc_data.shape[0].compute()\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "\n",
    "print(f\"Number of columns: {num_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will remove some of the columns that we do not need for our first analysis.\n",
    "\n",
    "# For now we will not consider peak events,so we do not need Event_Key column\n",
    "\n",
    "#Since much of the PV generation and controlled tariff is 0 we will ignore these columns as well and focus our attention on general supply.\n",
    "\n",
    "columns_to_drop = [' EVENT_KEY', ' CONTROLLED_LOAD_KWH', ' GROSS_GENERATION_KWH', ' NET_GENERATION_KWH', ' OTHER_KWH']\n",
    "sgsc_data = sgsc_data.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgsc_data['READING_DATETIME'] = dd.to_datetime(sgsc_data['READING_DATETIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data types:\n",
      " CUSTOMER_ID                   float64\n",
      "READING_DATETIME       datetime64[ns]\n",
      " CALENDAR_KEY                 float64\n",
      " GENERAL_SUPPLY_KWH           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Display initial data types\n",
    "print(\"Original data types:\\n\", sgsc_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract datetime components\n",
    "sgsc_data['year'] = sgsc_data['READING_DATETIME'].dt.year.astype('int16')\n",
    "sgsc_data['month'] = sgsc_data['READING_DATETIME'].dt.month.astype('int8')\n",
    "sgsc_data['day'] = sgsc_data['READING_DATETIME'].dt.day.astype('int8')\n",
    "sgsc_data['hour'] = sgsc_data['READING_DATETIME'].dt.hour.astype('int8')\n",
    "sgsc_data['minute'] = sgsc_data['READING_DATETIME'].dt.minute.astype('int8')\n",
    "sgsc_data['dayofyear'] = sgsc_data['READING_DATETIME'].dt.dayofyear.astype('int16')\n",
    "sgsc_data['dayofweek'] = sgsc_data['READING_DATETIME'].dt.dayofweek.astype('int8')\n",
    "sgsc_data['is_weekend'] = sgsc_data['dayofweek'].isin([5, 6]).astype('bool')\n",
    "sgsc_data[' GENERAL_SUPPLY_KWH'] = sgsc_data[' GENERAL_SUPPLY_KWH'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create holidays column\n",
    "au_holidays = holidays.Australia()\n",
    "\n",
    "# Since we're using Dask, we need to create a function to check for holidays\n",
    "def is_holiday(date):\n",
    "    return date in au_holidays\n",
    "\n",
    "# Add is_holiday column using map_partitions\n",
    "sgsc_data['is_holiday'] = sgsc_data['READING_DATETIME'].map_partitions(lambda x: x.map(is_holiday))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the total number of rows\n",
    "total_rows = len(sgsc_data.compute())\n",
    "print(f\"Total number of rows in the dataset: {total_rows}\")\n",
    "\n",
    "# Display the first few rows to verify the new columns\n",
    "print(\"\\nFirst few rows of the processed dataset:\")\n",
    "print(sgsc_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgsc_data['is_holiday'] = sgsc_data['is_holiday'].astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized data types:\n",
      " CUSTOMER_ID                   float64\n",
      "READING_DATETIME       datetime64[ns]\n",
      " CALENDAR_KEY                 float64\n",
      " GENERAL_SUPPLY_KWH           float32\n",
      "year                            int16\n",
      "month                            int8\n",
      "day                              int8\n",
      "hour                             int8\n",
      "minute                           int8\n",
      "dayofyear                       int16\n",
      "dayofweek                        int8\n",
      "is_weekend                       bool\n",
      "is_holiday                       bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOptimized data types:\\n\", sgsc_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original READING_DATETIME column\n",
    "sgsc_data = sgsc_data.drop('READING_DATETIME', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CUSTOMER_ID', ' CALENDAR_KEY', ' GENERAL_SUPPLY_KWH', 'year', 'month',\n",
      "       'day', 'hour', 'minute', 'dayofyear', 'dayofweek', 'is_weekend',\n",
      "       'is_holiday'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(sgsc_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "\n",
    "missing_customer_id = sgsc_data['CUSTOMER_ID'].isnull().sum().compute()\n",
    "missing_general_supply_kwh = sgsc_data[' GENERAL_SUPPLY_KWH'].isnull().sum().compute()\n",
    "missing_kalender_key = sgsc_data[' CALENDAR_KEY'].isnull().sum().compute()\n",
    "\n",
    "\n",
    "print(f\"Missing values in CUSTOMER_ID: {missing_customer_id}\")\n",
    "print(f\"Missing values in GENERAL_SUPPLY_KWH: {missing_general_supply_kwh}\")\n",
    "print(f\"Missing values in CUSTOMER_ID: {missing_kalender_key}\")\n",
    "\n",
    "# There is no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_to_hourly(df):\n",
    "    \"\"\"\n",
    "    Aggregate half-hourly energy consumption data to hourly by summing up the energy consumption\n",
    "    for each customer for each hour.\n",
    "    \n",
    "    Parameters:\n",
    "    df (dask.dataframe.DataFrame): Input DataFrame with half-hourly data\n",
    "    \n",
    "    Returns:\n",
    "    dask.dataframe.DataFrame: Aggregated hourly data\n",
    "    \"\"\"\n",
    "    # Group by all relevant columns except minute and CALENDAR_KEY\n",
    "    grouped = df.groupby([\n",
    "        'CUSTOMER_ID',\n",
    "        'year',\n",
    "        'month',\n",
    "        'day',\n",
    "        'hour',\n",
    "        'dayofyear',\n",
    "        'dayofweek',\n",
    "        'is_weekend',\n",
    "        'is_holiday'\n",
    "    ])\n",
    "    \n",
    "    # Sum up the energy consumption\n",
    "    aggregated = grouped.agg({\n",
    "        ' GENERAL_SUPPLY_KWH': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data = aggregate_to_hourly(sgsc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data = hourly_data.sort_values(\n",
    "    by=['CUSTOMER_ID', 'year', 'month', 'day', 'hour']\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\FL Publication\\\\Datasets for the Publication\\\\Australia - Smart-Grid Smart-City Customer Trial Data\\\\electricity_use_interval_readings\\\\electricity_use_hourly.csv']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_data.to_csv('D:/FL Publication/Datasets for the Publication/Australia - Smart-Grid Smart-City Customer Trial Data/electricity_use_interval_readings/electricity_use_hourly.csv', single_file=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "federated_learning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
